{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How Sentence Transformers models work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Sentence Transformer model, you map a variable-length text (or image pixels) to a fixed-size embedding representing that input's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the Sentence Transformers models work:\n",
    "\n",
    "1. Layer 1 â€“ The input text is passed through a pre-trained Transformer model that can be obtained directly from the Hugging Face Hub. This tutorial will use the \"distilroberta-base\" model. The Transformer outputs are contextualized word embeddings for all input tokens; imagine an embedding for each token of the text.\n",
    "\n",
    "2. Layer 2 - The embeddings go through a pooling layer to get a single fixed-length embedding for all the text. For example, mean pooling averages the embeddings generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "## Step 1: use an existing language model\n",
    "word_embedding_model = models.Transformer('distilroberta-base')\n",
    "\n",
    "## Step 2: use a pool function over the token embeddings\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "## Join steps 1 and 2 using the modules argument\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The question I have here is: Tutorial says that from first layer we get last hidden state of transformer. So let's check it comparing with output of the original transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download original RobertaModel and its tokenizator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "distilroberta_model = RobertaModel.from_pretrained('distilroberta-base')\n",
    "from transformers import RobertaTokenizer\n",
    "distilroberta_tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use next sentence. First we will tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1='I am a student'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_emb=distilroberta_tokenizer(text1,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  100,  524,   10, 1294,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]]), 'token_embeddings': tensor([[[ 0.0008,  0.0691, -0.0220,  ..., -0.0082, -0.0437, -0.0104],\n",
       "         [ 0.0038,  0.1006, -0.0765,  ...,  0.1998, -0.0285, -0.1054],\n",
       "         [ 0.1853,  0.0873,  0.0098,  ...,  0.3601,  0.0253, -0.1086],\n",
       "         [ 0.2156, -0.1720, -0.1061,  ...,  0.2778,  0.0912,  0.0706],\n",
       "         [ 0.0728, -0.0749, -0.0756,  ...,  0.1787,  0.1348,  0.0565],\n",
       "         [-0.0155,  0.0790, -0.0671,  ..., -0.0799, -0.0534, -0.0420]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will look at the output of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilroberta_model.eval()\n",
    "out = distilroberta_model(input_ids=text_emb['input_ids'],\n",
    "                 attention_mask=text_emb['attention_mask'],\n",
    "                 output_attentions=True,\n",
    "                 output_hidden_states=True,\n",
    "                 return_dict=True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0008,  0.0691, -0.0220,  ..., -0.0082, -0.0437, -0.0104],\n",
       "         [ 0.0038,  0.1006, -0.0765,  ...,  0.1998, -0.0285, -0.1054],\n",
       "         [ 0.1853,  0.0873,  0.0098,  ...,  0.3601,  0.0253, -0.1086],\n",
       "         [ 0.2156, -0.1720, -0.1061,  ...,  0.2778,  0.0912,  0.0706],\n",
       "         [ 0.0728, -0.0749, -0.0756,  ...,  0.1787,  0.1348,  0.0565],\n",
       "         [-0.0155,  0.0790, -0.0671,  ..., -0.0799, -0.0534, -0.0420]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1637,  0.0231, -0.1608, -0.2013,  0.2004], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['pooler_output'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the output of the transformer module from sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_embedding_model.eval()\n",
    "out = word_embedding_model(text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  100,  524,   10, 1294,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]]), 'token_embeddings': tensor([[[ 0.0008,  0.0691, -0.0220,  ..., -0.0082, -0.0437, -0.0104],\n",
       "         [ 0.0038,  0.1006, -0.0765,  ...,  0.1998, -0.0285, -0.1054],\n",
       "         [ 0.1853,  0.0873,  0.0098,  ...,  0.3601,  0.0253, -0.1086],\n",
       "         [ 0.2156, -0.1720, -0.1061,  ...,  0.2778,  0.0912,  0.0706],\n",
       "         [ 0.0728, -0.0749, -0.0756,  ...,  0.1787,  0.1348,  0.0565],\n",
       "         [-0.0155,  0.0790, -0.0671,  ..., -0.0799, -0.0534, -0.0420]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it also outputs the tokenized text. Note that its tokenized output is the same as output of the original tokenizator. Also note that the output is really the last_hidden_state of the original trasformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The next question is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not use a Transformer model, like BERT or Roberta, out of the box to create embeddings for entire sentences and texts? There are at least two reasons.\n",
    "\n",
    "1. Pre-trained Transformers require heavy computation to perform semantic search tasks. For example, finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. In contrast, a BERT Sentence Transformers model reduces the time to about 5 seconds.\n",
    "\n",
    "2. Once trained, Transformers create poor sentence representations out of the box. A BERT model with its token embeddings averaged to create a sentence embedding performs worse than the GloVe embeddings developed in 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Sentence Transformers model, you need to inform it somehow that two sentences have a certain degree of similarity. Therefore, each example in the data requires a label or structure that allows the model to understand whether two sentences are similar or different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is no single way to prepare your data to train a Sentence Transformers model. Furthermore, the structure of your data will influence which loss function you can use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most dataset configurations will take one of four forms (below you will see examples of each case):\n",
    "\n",
    "1. Case 1: A pair of sentences and label. The label can be integer o float.\n",
    "\n",
    "2. Case 2: The example is a pair of positive (similar) sentences without a label. For example, pairs of paraphrases, pairs of full texts and their summaries, pairs of duplicate questions, pairs of (query, response), or pairs of (source_language, target_language). Natural Language Inference datasets can also be formatted this way by pairing entailing sentences. Having your data in this format can be great since you can use the MultipleNegativesRankingLoss, one of the most used loss functions for Sentence Transformers models.\n",
    "\n",
    "3. Case 3: The example is a sentence with an integer label. This data format is easily converted by loss functions into three sentences (triplets) where the first is an \"anchor\", the second a \"positive\" of the same class as the anchor, and the third a \"negative\" of a different class. Each sentence has an integer label indicating the class to which it belongs.\n",
    "\n",
    "4. Case 4: The example is a triplet (anchor, positive, negative) without classes or labels for the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is converting the dataset into a format the Sentence Transformers model can understand. The model cannot accept raw lists of strings. Each example must be converted to a sentence_transformers.InputExample class and then to a torch.utils.data.DataLoader class to batch and shuffle the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download dataset for Case4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_id = \"embedding-data/QQP_triplets\"\n",
    "dataset = load_dataset(dataset_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we can see the Case4 structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'Why in India do we not have one on one political debate as in USA?',\n",
       "  'pos': ['Why cant we have a public debate between politicians in India like the one in US?'],\n",
       "  'neg': ['Can people on Quora stop India Pakistan debate? We are sick and tired seeing this everyday in bulk?',\n",
       "   'Why do politicians, instead of having a decent debate on issues going in and around the world, end up fighting always?',\n",
       "   'Can educated politicians make a difference in India?',\n",
       "   'What are some unusual aspects about politics and government in India?',\n",
       "   'What is debate?',\n",
       "   'Why does civic public communication and discourse seem so hollow in modern India?',\n",
       "   'What is a Parliamentary debate?',\n",
       "   \"Why do we always have two candidates at the U.S. presidential debate. yet the ballot has about 7 candidates? Isn't that a misrepresentation of democracy?\",\n",
       "   'Why is civic public communication and discourse so hollow in modern India?',\n",
       "   \"Aren't the Presidential debates teaching our whole country terrible communication skills and why is deliberate misrepresentation even allowed?\",\n",
       "   'Why are most Indian politicians uneducated?',\n",
       "   'Does Indian political leaders capable of doing face to face debates while running for office?',\n",
       "   'What is wrong with the Indian political system and the environment it has built in connection with the people of India? Have parties divided people more?',\n",
       "   'What is a debate?',\n",
       "   'Why do we have legislative council in india?',\n",
       "   'Why does the office of president of India, being politically neutral, not ask for Population control in India?',\n",
       "   \"Why don't we discuss tax and foreign policies more in Indian elections but are instead obsessed with socialist schemes?\",\n",
       "   'Why do Indian politicians lack nationalist thinking?',\n",
       "   'Do you hate indian politicians?',\n",
       "   'Is India facing more stessful times and politically charged atmosphere when compared to Congress regime?',\n",
       "   'Who is the best politician in India? Why?',\n",
       "   \"We all know about the present condition of Indian politicians; they are all just using us to run their train, but still, they win elections and rule over us. Why aren't people giving their vote to NOTA?\",\n",
       "   'Who are clean politicians in India?',\n",
       "   'Why are you not believing in Democracy of India?',\n",
       "   'What does politics in India mean? What are they actually doing?',\n",
       "   'What are the strongest arguments for a debate in favour of brain drain in India and what sources must be used for making a good short speech?',\n",
       "   'Do we really need an election commission in India?',\n",
       "   'Why is there no concept of political correctness in India? Is it a good thing or a bad thing?',\n",
       "   'Why is population control not on agenda of any political party in India?',\n",
       "   'Who are some of the most dangerous or worst politicians in India?']}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['set'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will convert our dataset to InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = []\n",
    "train_data = dataset['train']['set']\n",
    "# For agility we only 1/2 of our available data\n",
    "n_examples = dataset['train'].num_rows // 2000\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_examples):\n",
    "  example = train_data[i]\n",
    "  train_examples.append(InputExample(texts=[example['query'], example['pos'][0], example['neg'][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the list of InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sentence_transformers.readers.InputExample.InputExample at 0x2305616add0>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x23001be4e90>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x23077572c90>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2300f87ebd0>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2300f87ec90>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2305e1d41d0>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2305e1d4210>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2305e1d4190>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2305e1d4150>,\n",
       " <sentence_transformers.readers.InputExample.InputExample at 0x2305e1d4110>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the training examples to a Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The next step is to choose a suitable loss function that can be used with the data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the four different formats your data could be in? Each will have a different loss function associated with it.\n",
    "\n",
    "1. Case 1: Pair of sentences and a label indicating how similar they are. The loss function optimizes such that (1) the sentences with the closest labels are near in the vector space, and (2) the sentences with the farthest labels are as far as possible. The loss function depends on the format of the label. If its an integer use ContrastiveLoss or SoftmaxLoss; if its a float you can use CosineSimilarityLoss.\n",
    "\n",
    "2. Case 2: If you only have two similar sentences (two positives) with no labels, then you can use the MultipleNegativesRankingLoss function. The MegaBatchMarginLoss can also be used, and it would convert your examples to triplets (anchor_i, positive_i, positive_j) where positive_j serves as the negative.\n",
    "\n",
    "3. Case 3: When your samples are triplets of the form [anchor, positive, negative] and you have an integer label for each, a loss function optimizes the model so that the anchor and positive sentences are closer together in vector space than the anchor and negative sentences. You can use BatchHardTripletLoss, which requires the data to be labeled with integers (e.g., labels 1, 2, 3) assuming that samples with the same label are similar. Therefore, anchors and positives must have the same label, while negatives must have a different one. Alternatively, you can use BatchAllTripletLoss, BatchHardSoftMarginTripletLoss, or BatchSemiHardTripletLoss. The differences between them is beyond the scope of this tutorial, but can be reviewed in the Sentence Transformers documentation.\n",
    "\n",
    "4. Case 4: If you don't have a label for each sentence in the triplets, you should use TripletLoss. This loss minimizes the distance between the anchor and the positive sentences while maximizing the distance between the anchor and the negative sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](losses_for_dif_structures.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we must use TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "train_loss = losses.TripletLoss(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What are the limits of Sentence Transformers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Transformers models work much better than the simple Transformers models for semantic search. However, where do the Sentence Transformers models not work well? If your task is classification, then using sentence embeddings is the wrong approach. In that case, the ðŸ¤— Transformers library would be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "evaluator = TripletEvaluator([train_data[i]['query'] for i in range(30)],[train_data[i]['pos'][0] for i in range(30)],[train_data[i]['neg'] for i in range(30)],batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d6188d53ef4e9bae46e4a9be3a0b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1fce4f4a6140df9a370c2fe261e6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4034310795574d20b16f9ea78f3d9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          evaluator=evaluator,\n",
    "          evaluation_steps=5,\n",
    "          warmup_steps=warmup_steps) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
